{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-23T15:45:22+01:00\n",
      "\n",
      "CPython 3.6.8\n",
      "IPython 7.5.0\n",
      "\n",
      "compiler   : GCC 7.3.0\n",
      "system     : Linux\n",
      "release    : 4.15.0-50-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 8\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask para Machine Learning\n",
    "\n",
    "Dask está integrado con scikit-learn, esto significa que nos puede ayudar a gestionar nuestros datasets de dos formas distintas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Usar Dask para entrenar modelos de datasets de small data usando un cluster. \n",
    "\n",
    "Es posible usar dask para acelerar el entrenamiento de modelos y la búsqueda de hiperparámetros para datasets que caben en memoria.\n",
    "\n",
    "Vamos a trabajar con el dataset de viajes en taxi en Nueva York del 2014.\n",
    "\n",
    "Primero exportamos el dataset a formato parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.read_csv(\"../data/nyc_taxi_data_2014.csv\").to_parquet(\"../data/nyc_taxi_data_2014.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos leerlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_2014 = dd.read_parquet(\"../data/nyc_taxi_data_2014.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>rate_code</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>surcharge</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CMT</td>\n",
       "      <td>2014-01-09 20:45:25</td>\n",
       "      <td>2014-01-09 20:52:31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-73.994770</td>\n",
       "      <td>40.736828</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.982227</td>\n",
       "      <td>40.731790</td>\n",
       "      <td>CRD</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CMT</td>\n",
       "      <td>2014-01-09 20:46:12</td>\n",
       "      <td>2014-01-09 20:55:12</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>-73.982392</td>\n",
       "      <td>40.773382</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.960449</td>\n",
       "      <td>40.763995</td>\n",
       "      <td>CRD</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CMT</td>\n",
       "      <td>2014-01-09 20:44:47</td>\n",
       "      <td>2014-01-09 20:59:46</td>\n",
       "      <td>2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>-73.988570</td>\n",
       "      <td>40.739406</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.986626</td>\n",
       "      <td>40.765217</td>\n",
       "      <td>CRD</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CMT</td>\n",
       "      <td>2014-01-09 20:44:57</td>\n",
       "      <td>2014-01-09 20:51:40</td>\n",
       "      <td>1</td>\n",
       "      <td>1.7</td>\n",
       "      <td>-73.960213</td>\n",
       "      <td>40.770464</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.979863</td>\n",
       "      <td>40.777050</td>\n",
       "      <td>CRD</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CMT</td>\n",
       "      <td>2014-01-09 20:47:09</td>\n",
       "      <td>2014-01-09 20:53:32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-73.995371</td>\n",
       "      <td>40.717248</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.984367</td>\n",
       "      <td>40.720524</td>\n",
       "      <td>CRD</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vendor_id      pickup_datetime     dropoff_datetime  passenger_count  \\\n",
       "0       CMT  2014-01-09 20:45:25  2014-01-09 20:52:31                1   \n",
       "1       CMT  2014-01-09 20:46:12  2014-01-09 20:55:12                1   \n",
       "2       CMT  2014-01-09 20:44:47  2014-01-09 20:59:46                2   \n",
       "3       CMT  2014-01-09 20:44:57  2014-01-09 20:51:40                1   \n",
       "4       CMT  2014-01-09 20:47:09  2014-01-09 20:53:32                1   \n",
       "\n",
       "   trip_distance  pickup_longitude  pickup_latitude  rate_code  \\\n",
       "0            0.7        -73.994770        40.736828          1   \n",
       "1            1.4        -73.982392        40.773382          1   \n",
       "2            2.3        -73.988570        40.739406          1   \n",
       "3            1.7        -73.960213        40.770464          1   \n",
       "4            0.9        -73.995371        40.717248          1   \n",
       "\n",
       "  store_and_fwd_flag  dropoff_longitude  dropoff_latitude payment_type  \\\n",
       "0                  N         -73.982227         40.731790          CRD   \n",
       "1                  N         -73.960449         40.763995          CRD   \n",
       "2                  N         -73.986626         40.765217          CRD   \n",
       "3                  N         -73.979863         40.777050          CRD   \n",
       "4                  N         -73.984367         40.720524          CRD   \n",
       "\n",
       "   fare_amount  surcharge  mta_tax  tip_amount  tolls_amount  total_amount  \n",
       "0          6.5        0.5      0.5        1.40           0.0          8.90  \n",
       "1          8.5        0.5      0.5        1.90           0.0         11.40  \n",
       "2         11.5        0.5      0.5        1.50           0.0         14.00  \n",
       "3          7.5        0.5      0.5        1.70           0.0         10.20  \n",
       "4          6.0        0.5      0.5        1.75           0.0          8.75  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_2014.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado vamos a ver como dask puede acelerar el entrenamiento de datasets que caben en memoria. Por lo tanto voy a coger una muestra del dataset para que funcione bien (mi ordenador es potente pero es un portatil :) )\n",
    "\n",
    "Podemos convertirlo a un dataframe de pandas haciendo `compute()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_2014 = taxi_2014.sample(frac=0.01, random_state=42).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149997, 18)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_2014.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un pipeline normal de scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import OneHotEncoder\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer una busqueda de hiperparametros para entrenar un modelo predictivo que prediga la propina de un viaje en taxi en funcion de las características del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_objetivo = \"tip_amount\"\n",
    "y = taxi_2014[variable_objetivo]\n",
    "X = taxi_2014[[\"vendor_id\", \"store_and_fwd_flag\", \"payment_type\",\n",
    "               \"rate_code\", \"pickup_longitude\", \"pickup_latitude\", \n",
    "               \"passenger_count\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    make_union(\n",
    "        make_pipeline(\n",
    "            ColumnSelector(cols=[\"vendor_id\", \"store_and_fwd_flag\", \"payment_type\", \"rate_code\"]),\n",
    "            OneHotEncoder()\n",
    "        ),\n",
    "        make_pipeline(\n",
    "            ColumnSelector(cols=[\"pickup_longitude\", \"pickup_latitude\", \"passenger_count\"]),\n",
    "            StandardScaler()\n",
    "        )  \n",
    "    ),\n",
    "    SGDRegressor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('featureunion', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('pipeline-1', Pipeline(memory=None,\n",
       "     steps=[('columnselector', ColumnSelector(cols=['vendor_id', 'store_and_fwd_flag', 'payment_type', 'rate_code'],\n",
       "        drop_axis=False)), ('onehotencoder', OneHotEncoder(cols=None, d...m_state=None, shuffle=True, tol=None, validation_fraction=0.1,\n",
       "       verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('featureunion', FeatureUnion(n_jobs=None,\n",
       "         transformer_list=[('pipeline-1', Pipeline(memory=None,\n",
       "       steps=[('columnselector', ColumnSelector(cols=['vendor_id', 'store_and_fwd_flag', 'payment_type', 'rate_code'],\n",
       "          drop_axis=False)), ('onehotencoder', OneHotEncoder(cols=None, drop_invariant=False, handle_unknown='impute',\n",
       "         impu... drop_axis=False)), ('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True))]))],\n",
       "         transformer_weights=None)),\n",
       " ('sgdregressor',\n",
       "  SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "         eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "         learning_rate='invscaling', loss='squared_loss', max_iter=None,\n",
       "         n_iter=None, n_iter_no_change=5, penalty='l2', power_t=0.25,\n",
       "         random_state=None, shuffle=True, tol=None, validation_fraction=0.1,\n",
       "         verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer ahora una búsqueda de hiperparámetros, pero vamos a usar dask para acelerar dicha busqueda. De dicha forma podremos explorar los parámetros de forma distribuida (con muchos ordenadores a la vez), lo que reduce el tiempo de búsqueda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:40911\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>67.50 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:40911' processes=4 cores=8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos la lista de parámetros disponible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['featureunion',\n",
       " 'featureunion__n_jobs',\n",
       " 'featureunion__pipeline-1',\n",
       " 'featureunion__pipeline-1__columnselector',\n",
       " 'featureunion__pipeline-1__columnselector__cols',\n",
       " 'featureunion__pipeline-1__columnselector__drop_axis',\n",
       " 'featureunion__pipeline-1__memory',\n",
       " 'featureunion__pipeline-1__onehotencoder',\n",
       " 'featureunion__pipeline-1__onehotencoder__cols',\n",
       " 'featureunion__pipeline-1__onehotencoder__drop_invariant',\n",
       " 'featureunion__pipeline-1__onehotencoder__handle_unknown',\n",
       " 'featureunion__pipeline-1__onehotencoder__impute_missing',\n",
       " 'featureunion__pipeline-1__onehotencoder__return_df',\n",
       " 'featureunion__pipeline-1__onehotencoder__use_cat_names',\n",
       " 'featureunion__pipeline-1__onehotencoder__verbose',\n",
       " 'featureunion__pipeline-1__steps',\n",
       " 'featureunion__pipeline-2',\n",
       " 'featureunion__pipeline-2__columnselector',\n",
       " 'featureunion__pipeline-2__columnselector__cols',\n",
       " 'featureunion__pipeline-2__columnselector__drop_axis',\n",
       " 'featureunion__pipeline-2__memory',\n",
       " 'featureunion__pipeline-2__standardscaler',\n",
       " 'featureunion__pipeline-2__standardscaler__copy',\n",
       " 'featureunion__pipeline-2__standardscaler__with_mean',\n",
       " 'featureunion__pipeline-2__standardscaler__with_std',\n",
       " 'featureunion__pipeline-2__steps',\n",
       " 'featureunion__transformer_list',\n",
       " 'featureunion__transformer_weights',\n",
       " 'memory',\n",
       " 'sgdregressor',\n",
       " 'sgdregressor__alpha',\n",
       " 'sgdregressor__average',\n",
       " 'sgdregressor__early_stopping',\n",
       " 'sgdregressor__epsilon',\n",
       " 'sgdregressor__eta0',\n",
       " 'sgdregressor__fit_intercept',\n",
       " 'sgdregressor__l1_ratio',\n",
       " 'sgdregressor__learning_rate',\n",
       " 'sgdregressor__loss',\n",
       " 'sgdregressor__max_iter',\n",
       " 'sgdregressor__n_iter',\n",
       " 'sgdregressor__n_iter_no_change',\n",
       " 'sgdregressor__penalty',\n",
       " 'sgdregressor__power_t',\n",
       " 'sgdregressor__random_state',\n",
       " 'sgdregressor__shuffle',\n",
       " 'sgdregressor__tol',\n",
       " 'sgdregressor__validation_fraction',\n",
       " 'sgdregressor__verbose',\n",
       " 'sgdregressor__warm_start',\n",
       " 'steps']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(pipeline.get_params().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procedemos a hacer la búsqueda aleatoria, pero en vez de usar sklearn vamos a usar [dask_searchcv](https://dask-searchcv.readthedocs.io/en/latest/). Dicho paquete se install haciendo `conda install dask-searchcv -c conda-forge`\n",
    "\n",
    "Tiene el mismo aspecto, pero usa dask como el gestor del entrenamiento.\n",
    "Como estoy usando un solo ordenador no va a proporcionar mucho valor pero si tuviesemos un cluster se notaría aun más la diferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dask_searchcv import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "param_dist = {\n",
    "    \"sgdregressor__penalty\": [\"l1\", \"l2\", \"elasticnet\"],\n",
    "    \"sgdregressor__learning_rate\": [\"constant\", \"optimal\", \"invscaling\", \"adaptive\"],\n",
    "    \"sgdregressor__loss\": [\"squared_loss\", \"epsilon_insensitive\"],\n",
    "}\n",
    "\n",
    "\n",
    "search = RandomizedSearchCV(pipeline, param_dist, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37 s, sys: 8.75 s, total: 45.7 s\n",
      "Wall time: 25.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cache_cv=True, cv=3, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('featureunion', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('pipeline-1', Pipeline(memory=None,\n",
       "     steps=[('columnselector', ColumnSelector(cols=['vendor_id', 'store_and_fwd_flag', 'payment_type', 'rate_code'],\n",
       "        drop_axis=False)), ('onehotencoder', OneHotEncoder(cols=None, d...m_state=None, shuffle=True, tol=None, validation_fraction=0.1,\n",
       "       verbose=0, warm_start=False))]),\n",
       "          iid=True, n_iter=10, n_jobs=-1,\n",
       "          param_distributions={'sgdregressor__penalty': ['l1', 'l2', 'elasticnet'], 'sgdregressor__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'], 'sgdregressor__loss': ['squared_loss', 'epsilon_insensitive']},\n",
       "          random_state=None, refit=True, return_train_score='warn',\n",
       "          scheduler=None, scoring=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2248068852281419"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "       eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='invscaling', loss='epsilon_insensitive',\n",
       "       max_iter=None, n_iter=None, n_iter_no_change=5,\n",
       "       penalty='elasticnet', power_t=0.25, random_state=None, shuffle=True,\n",
       "       tol=None, validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_estimator_.get_params()[\"sgdregressor\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer la misma busqueda con scikit learn para ver las diferencias en tiempo de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV as SKRandomizedSearchCV\n",
    "sk_search = SKRandomizedSearchCV(pipeline, param_dist, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.3 s, sys: 14.1 s, total: 1min 12s\n",
      "Wall time: 51.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('featureunion', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('pipeline-1', Pipeline(memory=None,\n",
       "     steps=[('columnselector', ColumnSelector(cols=['vendor_id', 'store_and_fwd_flag', 'payment_type', 'rate_code'],\n",
       "        drop_axis=False)), ('onehotencoder', OneHotEncoder(cols=None, d...m_state=None, shuffle=True, tol=None, validation_fraction=0.1,\n",
       "       verbose=0, warm_start=False))]),\n",
       "          fit_params=None, iid='warn', n_iter=10, n_jobs=None,\n",
       "          param_distributions={'sgdregressor__penalty': ['l1', 'l2', 'elasticnet'], 'sgdregressor__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'], 'sgdregressor__loss': ['squared_loss', 'epsilon_insensitive']},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time sk_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que usar la búsqueda de dask acelera aun usando solo nuestro ordenador. Ésto es asi por que [dask es más inteligente que sckit-learn a la hora de decidir como hacer los pasos necesarios para la búsqueda](https://dask-ml.readthedocs.io/en/latest/hyper-parameter-search.html)\n",
    "\n",
    "Scikit learn hace todos los pasos de la búsqueda para cada combinación de hiperparámetros,por ejemplo para la siguiente búsqueda de malla de un pipeline:\n",
    "\n",
    "```\n",
    "pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier())])\n",
    "\n",
    "grid = {'vect__ngram_range': [(1, 1)],\n",
    "        'tfidf__norm': ['l1', 'l2'],\n",
    "        'clf__alpha': [1e-3, 1e-4, 1e-5]}\n",
    "```\n",
    "\n",
    "Scikit-learn hace ésto:\n",
    "![](https://dask-ml.readthedocs.io/en/latest/_images/unmerged_grid_search_graph.svg)\n",
    "\n",
    "Dask sabe que hay pasos que se pueden reutilizar (sin tener que recalcularlos) y hace un DAG (Directed Acyclic Graph) optimizado:\n",
    "\n",
    "![](https://dask-ml.readthedocs.io/en/latest/_images/merged_grid_search_graph.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Usar dask para entrenar modelos de scikit-learn de datasets que no caben en memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos el paquete [dask-ml](https://ml.dask.org/) que contiene implementaciones de algoritmos de scikitlearn pero que funcionan con dask. Se instala de la forma habitual, haciendo:\n",
    "\n",
    "`conda install -c conda-forge dask-ml` (desde la terminal como siempre)\n",
    "\n",
    "dicho paquete contiene implementaciones de modelos de ML que pueden trabajar de forma paralela con un cluster usando dask. De dicha forma se pueden entrenar modelos con datos que no caben en memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.linear_model import PartialSGDRegressor, PartialSGDClassifier, LogisticRegression\n",
    "from dask_ml.preprocessing import StandardScaler, OneHotEncoder, DummyEncoder, Categorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a trabajar ahora con el dataset de 2017, recordemos que tiene 113 millones de filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_2017 = dd.read_parquet(\"../data/2017_Yellow_Taxi_Trip_Data.parquet/*.parquet\"\n",
    "                           ).categorize([\"VendorID\"])\n",
    "taxi_2017 = taxi_2017[taxi_2017[\"tip_amount\"] >=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = taxi_2017[[\"n_pasajeros\", \"distancia\", \"VendorID\"]]\n",
    "y = taxi_2017[variable_objetivo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_pasajeros</th>\n",
       "      <th>distancia</th>\n",
       "      <th>VendorID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=165</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>category[known]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: getitem, 990 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                n_pasajeros distancia         VendorID\n",
       "npartitions=165                                       \n",
       "                    float64   float64  category[known]\n",
       "                        ...       ...              ...\n",
       "...                     ...       ...              ...\n",
       "                        ...       ...              ...\n",
       "                        ...       ...              ...\n",
       "Dask Name: getitem, 990 tasks"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos el pipeline, pero usando cuando es posible los transformadores de `dask-ml`, hay que hacer ciertas manipulaciones para asegurarse de que trabaja con elementos de dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from dask_ml.impute import SimpleImputer\n",
    "from dask_ml.linear_model import LinearRegression\n",
    "from dask_ml.wrappers import Incremental\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "import dask.array as da\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_columns = X.columns.tolist()\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    FunctionTransformer(lambda x:  dd.from_array(x, columns=ind_columns), validate=False),\n",
    "    Categorizer(categories={\"VendorID\": [[1,2], False]}),\n",
    "    DummyEncoder(columns=[\"VendorID\"]),\n",
    "    FunctionTransformer(lambda x:  x.values, validate=False),\n",
    "    Incremental(SGDRegressor())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('functiontransformer-1',\n",
       "  FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "            func=<function <lambda> at 0x7f6914087378>, inv_kw_args=None,\n",
       "            inverse_func=None, kw_args=None, pass_y='deprecated',\n",
       "            validate=False)),\n",
       " ('categorizer',\n",
       "  Categorizer(categories={'VendorID': [[1, 2], False]}, columns=None)),\n",
       " ('dummyencoder', DummyEncoder(columns=['VendorID'], drop_first=False)),\n",
       " ('functiontransformer-2',\n",
       "  FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "            func=<function <lambda> at 0x7f6914087ea0>, inv_kw_args=None,\n",
       "            inverse_func=None, kw_args=None, pass_y='deprecated',\n",
       "            validate=False)),\n",
       " ('incremental',\n",
       "  Incremental(estimator=SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "         eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "         learning_rate='invscaling', loss='squared_loss', max_iter=None,\n",
       "         n_iter=None, n_iter_no_change=5, penalty='l2', power_t=0.25,\n",
       "         random_state=None, shuffle=True, tol=None, validation_fraction=0.1,\n",
       "         verbose=0, warm_start=False),\n",
       "        random_state=None, scoring=None, shuffle_blocks=True))]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ajustamos el pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.7 s, sys: 2.18 s, total: 24.8 s\n",
      "Wall time: 2min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('functiontransformer-1', FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "          func=<function <lambda> at 0x7f6914087378>, inv_kw_args=None,\n",
       "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
       "          validate=False)), ('categorizer', Categorizer(categories={'V...,\n",
       "       verbose=0, warm_start=False),\n",
       "      random_state=None, scoring=None, shuffle_blocks=True))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time pipeline.fit(X.values, y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha tardado 2 minutos, no está mal para un dataset que inicialmente era un csv que ocupaba 10 GB!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos usar `predict` con un array de dask como input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.array<values, shape=(nan, 3), dtype=object, chunksize=(nan, 3)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3.314809\n",
       "1    1.116306\n",
       "2    2.379460\n",
       "3    1.206138\n",
       "4    1.747883\n",
       "5    1.751656\n",
       "6    0.958237\n",
       "7    1.333236\n",
       "8    2.430601\n",
       "9    3.484720\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict(X.values).to_dask_dataframe().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "podemos cargar el dataset de 2014 y generar predicciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_2014 =  (dd\n",
    "      .read_parquet(\"../data/nyc_taxi_data_2014.parquet/\")\n",
    "      .rename(columns={\"vendor_id\": \"VendorID\"})\n",
    "      .categorize([\"VendorID\"])\n",
    "     )[[\"passenger_count\", \"trip_distance\", \"VendorID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>VendorID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>CMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>CMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>CMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.7</td>\n",
       "      <td>CMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>CMT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   passenger_count  trip_distance VendorID\n",
       "0                1            0.7      CMT\n",
       "1                1            1.4      CMT\n",
       "2                2            2.3      CMT\n",
       "3                1            1.7      CMT\n",
       "4                1            0.9      CMT"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_2014.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    CMT\n",
       "1    VTS\n",
       "Name: VendorID, dtype: category\n",
       "Categories (2, object): [CMT, VTS]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_2014.VendorID.unique().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_2014 = taxi_2014.assign(VendorID=taxi_2014.VendorID.str.replace('CMT', \"1\"))\n",
    "taxi_2014 = taxi_2014.assign(VendorID=taxi_2014.VendorID.str.replace('VTS', \"2\"))\n",
    "taxi_2014 = taxi_2014.assign(VendorID=taxi_2014.VendorID.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones = pipeline.predict(taxi_2014.values).to_dask_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.958412\n",
       "1    1.234727\n",
       "2    1.555958\n",
       "3    1.353148\n",
       "4    1.037359\n",
       "5    1.037359\n",
       "6    2.103145\n",
       "7    1.511042\n",
       "8    2.024198\n",
       "9    1.589989\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicciones.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este pipeline no solo funciona con dataframes de dask, sino que tambien funciona con dataframes de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertimos el dataset de dask a dataset de pandas\n",
    "taxi_2014 = taxi_2014.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(taxi_2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones = pipeline.predict(taxi_2014.values).to_dask_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.958412\n",
       "1    1.234727\n",
       "2    1.555958\n",
       "3    1.353148\n",
       "4    1.037359\n",
       "5    1.037359\n",
       "6    2.103145\n",
       "7    1.511042\n",
       "8    2.024198\n",
       "9    1.589989\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicciones.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
