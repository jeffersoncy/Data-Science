{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-25T16:58:26+01:00\n",
      "\n",
      "CPython 3.6.8\n",
      "IPython 7.5.0\n",
      "\n",
      "compiler   : GCC 7.3.0\n",
      "system     : Linux\n",
      "release    : 4.15.0-50-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 8\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark tiene varios modulos, entre ellos un modulo de [Machine Learning](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html), vamos a ver como usarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una sesion de spark, uso 6 cores por que si uso mas me quedo sin memoria en mi ordenador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.70:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[6]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Taxis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f35d8f32eb8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "     .master(\"local[6]\") \\\n",
    "     .appName(\"Taxis\") \\\n",
    "     .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a replicar el pipeline que usamos con dask\n",
    "\n",
    "```\n",
    "pipeline = make_pipeline(\n",
    "    make_union(\n",
    "        make_pipeline(\n",
    "            ColumnSelector(cols=[\"vendor_id\", \"store_and_fwd_flag\", \"payment_type\", \"rate_code\"]),\n",
    "            OneHotEncoder()\n",
    "        ),\n",
    "        make_pipeline(\n",
    "            ColumnSelector(cols=[\"pickup_longitude\", \"pickup_latitude\", \"passenger_count\"]),\n",
    "            StandardScaler()\n",
    "        )  \n",
    "    ),\n",
    "    SGDRegressor()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_objetivo = \"tip_amount\"\n",
    "variables_independientes = [\"vendor_id\", \"store_and_fwd_flag\", \"payment_type\",\n",
    "               \"rate_code\", \"pickup_longitude\", \"pickup_latitude\", \n",
    "               \"passenger_count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a leer el dataset de 2014 de trayectos de taxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = (\n",
    "    spark\n",
    "    .read\n",
    "    .parquet(\"../data/nyc_taxi_data_2014.parquet/\")\n",
    "    .select(variables_independientes + [variable_objetivo])\n",
    "    .withColumnRenamed(variable_objetivo, \"label\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-73.943106, pickup_latitude=40.706701, passenger_count=1, label=1.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark tiene la figura de [Pipelines](https://spark.apache.org/docs/latest/ml-pipeline.html) que es similar a la de scikit-learn, en particular define 2 tipos de objetos:\n",
    "\n",
    "- **Transformers**, que transforman el dataset de entrenamiento (encoders, standardizadores, etc)\n",
    "- **Estimators**, que realizan las prediciones (Regresion lineal, Random Forest, etc.\n",
    "\n",
    "Los transformadores de Spark funcionan con **1 sola columna como input** y **1 columna como output**.\n",
    "\n",
    "Los estimadores de spark funcionan con **1 columna como input** (llamada `features`) y **1 columna como output** (generalmente llamada `labels`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el pipeline vamos a usar los siguientes transformadores:\n",
    "    \n",
    "- `StringIndexer`, que convierte una columna de strings a numeros\n",
    "- `OneHotEncoder`, que realiza codificaci√≥n one hot de enteros\n",
    "- `StandardScaler`, que estandariza un vector de numeros\n",
    "- `VectorAssembler`, que toma un conjunto de columnas y las agrupa en una sola columna de vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, StandardScaler, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_categoricas = [\"vendor_id\", \"store_and_fwd_flag\", \"payment_type\", \"rate_code\"]\n",
    "variables_numericas = [\"pickup_longitude\", \"pickup_latitude\", \"passenger_count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "el StringIndexer y OneHotEncoder de spark trabajan con una sola columna, por lo tanto tenemos que crear  una lista de indexadores y encoders (uno de cada para cada columna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = []\n",
    "encoders = []\n",
    "for columna in variables_categoricas:\n",
    "    indexers.append(\n",
    "        StringIndexer(inputCol=columna, outputCol=f\"{columna}_idx)\", handleInvalid=\"keep\"))\n",
    "    encoders.append(\n",
    "        OneHotEncoder(inputCol=f\"{columna}_idx)\", outputCol=f\"{columna}_enc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_0355836cdcfd,\n",
       " StringIndexer_09e1ed4bde22,\n",
       " StringIndexer_b9584fc7d362,\n",
       " StringIndexer_884a41018097]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OneHotEncoder_70fb743ff741,\n",
       " OneHotEncoder_fb43e9837251,\n",
       " OneHotEncoder_c3962db20440,\n",
       " OneHotEncoder_f841bd12d78d]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vendor_id_idx)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoders[0].getInputCol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vendor_id_enc'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoders[0].getOutputCol()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos un [VectorAssembler](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler) que tomara el output de los encoders y los convertira en un vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensamblador_categorico = VectorAssembler(\n",
    "                            inputCols=[enc.getOutputCol() for enc in encoders], \n",
    "                            outputCol=\"vector_categorico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_882866db619c"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensamblador_categorico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos un pipeline para datos categoricos, que va a ser una lista con los siguentes pasos (o `stages` como se llaman en spark):\n",
    "\n",
    "1. indexers, que convierte strings de variables categoricas a numeros\n",
    "\n",
    "2.encoders, que convierten strings a codificacion one hot\n",
    "\n",
    "3. ensamblador, que  cogen el output de distintos encoders y los unen en un vector unico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_categorico = Pipeline(stages=indexers +encoders + [ensamblador_categorico])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.Pipeline"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pipeline_categorico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos evaluar el pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_cat = pipeline_categorico.fit(taxi).transform(taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[vendor_id: string, store_and_fwd_flag: string, payment_type: string, rate_code: bigint, pickup_longitude: double, pickup_latitude: double, passenger_count: bigint, label: double, vendor_id_idx): double, store_and_fwd_flag_idx): double, payment_type_idx): double, rate_code_idx): double, vendor_id_enc: vector, store_and_fwd_flag_enc: vector, payment_type_enc: vector, rate_code_enc: vector, vector_categorico: vector]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-73.943106, pickup_latitude=40.706701, passenger_count=1, label=1.0, vendor_id_idx)=1.0, store_and_fwd_flag_idx)=0.0, payment_type_idx)=0.0, rate_code_idx)=0.0, vendor_id_enc=SparseVector(2, {1: 1.0}), store_and_fwd_flag_enc=SparseVector(2, {0: 1.0}), payment_type_enc=SparseVector(5, {0: 1.0}), rate_code_enc=SparseVector(12, {0: 1.0}), vector_categorico=SparseVector(21, {1: 1.0, 2: 1.0, 4: 1.0, 9: 1.0})),\n",
       " Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-74.006878, pickup_latitude=40.739494, passenger_count=1, label=2.4, vendor_id_idx)=1.0, store_and_fwd_flag_idx)=0.0, payment_type_idx)=0.0, rate_code_idx)=0.0, vendor_id_enc=SparseVector(2, {1: 1.0}), store_and_fwd_flag_enc=SparseVector(2, {0: 1.0}), payment_type_enc=SparseVector(5, {0: 1.0}), rate_code_enc=SparseVector(12, {0: 1.0}), vector_categorico=SparseVector(21, {1: 1.0, 2: 1.0, 4: 1.0, 9: 1.0}))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_cat.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables num√©ricas**\n",
    "\n",
    "Para las variables numericas simplemente creamos un standard scaler las variables numericas.\n",
    "\n",
    "El [StandardScaler](https://spark.apache.org/docs/latest/ml-features.html#standardscaler) de spark funciona con una lista de vectores, estandarizando cada variable. Por ello ponemos primero todas las variables numericas en un vector mediante el uso de un VectorAssembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensamblador_numerico = VectorAssembler(inputCols=variables_numericas, \n",
    "                                       outputCol=\"vector_numerico\")    \n",
    "\n",
    "estandarizador = StandardScaler(inputCol=\"vector_numerico\", outputCol=\"vector_numerico_std\")    \n",
    "\n",
    "pipeline_numerico = Pipeline(stages=[ensamblador_numerico, estandarizador])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_num = pipeline_numerico.fit(taxi).transform(taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-73.943106, pickup_latitude=40.706701, passenger_count=1, label=1.0, vector_numerico=DenseVector([-73.9431, 40.7067, 1.0]), vector_numerico_std=DenseVector([-8.6117, 8.5471, 0.719])),\n",
       " Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-74.006878, pickup_latitude=40.739494, passenger_count=1, label=2.4, vector_numerico=DenseVector([-74.0069, 40.7395, 1.0]), vector_numerico_std=DenseVector([-8.6192, 8.554, 0.719]))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_num.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos un pipeline para variables categoricas y otro para variables numericas creamos un pipeline que sea la union del output de ambas, dicho output lo llamamos por convencion `features`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensamblador_procesado = VectorAssembler(inputCols=[\"vector_numerico\", \"vector_categorico\"], \n",
    "                                         outputCol=\"features\")  \n",
    "\n",
    "pipeline_procesado = Pipeline(stages=[pipeline_numerico, pipeline_categorico, \n",
    "                                      ensamblador_procesado])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_procesado = pipeline_procesado.fit(taxi).transform(taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-73.943106, pickup_latitude=40.706701, passenger_count=1, label=1.0, vector_numerico=DenseVector([-73.9431, 40.7067, 1.0]), vector_numerico_std=DenseVector([-8.6117, 8.5471, 0.719]), vendor_id_idx)=1.0, store_and_fwd_flag_idx)=0.0, payment_type_idx)=0.0, rate_code_idx)=0.0, vendor_id_enc=SparseVector(2, {1: 1.0}), store_and_fwd_flag_enc=SparseVector(2, {0: 1.0}), payment_type_enc=SparseVector(5, {0: 1.0}), rate_code_enc=SparseVector(12, {0: 1.0}), vector_categorico=SparseVector(21, {1: 1.0, 2: 1.0, 4: 1.0, 9: 1.0}), features=SparseVector(24, {0: -73.9431, 1: 40.7067, 2: 1.0, 4: 1.0, 5: 1.0, 7: 1.0, 12: 1.0})),\n",
       " Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-74.006878, pickup_latitude=40.739494, passenger_count=1, label=2.4, vector_numerico=DenseVector([-74.0069, 40.7395, 1.0]), vector_numerico_std=DenseVector([-8.6192, 8.554, 0.719]), vendor_id_idx)=1.0, store_and_fwd_flag_idx)=0.0, payment_type_idx)=0.0, rate_code_idx)=0.0, vendor_id_enc=SparseVector(2, {1: 1.0}), store_and_fwd_flag_enc=SparseVector(2, {0: 1.0}), payment_type_enc=SparseVector(5, {0: 1.0}), rate_code_enc=SparseVector(12, {0: 1.0}), vector_categorico=SparseVector(21, {1: 1.0, 2: 1.0, 4: 1.0, 9: 1.0}), features=SparseVector(24, {0: -74.0069, 1: 40.7395, 2: 1.0, 4: 1.0, 5: 1.0, 7: 1.0, 12: 1.0}))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_procesado.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora definimos el estimador, usaremos un modelo de regresion lineal por descenso de gradiente. \n",
    "\n",
    "Los estimadores de Spark, aparte de los hiperpar√°metros caracteristicos de cada algoritmo, tienen ciertos argumentos comunes:\n",
    "\n",
    "- `featuresCol`, el nombre de la columna de las variables independientes (por defecto, \"features\")\n",
    "- `labelCol`, el nombre de la columna de la variable independiente (por defecto, \"labels\")\n",
    "- `predictionCol`, el nombre para las predicciones (por defecto, \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(maxIter=100, regParam=0.3, elasticNetParam=0.8, \n",
    "                      featuresCol=\"features\", labelCol=\"label\",\n",
    "                      predictionCol=\"prediccion\")\n",
    "pipeline = Pipeline(stages=[pipeline_procesado, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hacemos `fit`, esto tarda unos 4 minutos en mi ordenador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 144 ms, sys: 29.3 ms, total: 173 ms\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%time modelo = pipeline.fit(taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_d2e6153fce94"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora para obtener predicciones hacemos `transform`, no predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-73.943106, pickup_latitude=40.706701, passenger_count=1, label=1.0, vector_numerico=DenseVector([-73.9431, 40.7067, 1.0]), vector_numerico_std=DenseVector([-8.6117, 8.5471, 0.719]), vendor_id_idx)=1.0, store_and_fwd_flag_idx)=0.0, payment_type_idx)=0.0, rate_code_idx)=0.0, vendor_id_enc=SparseVector(2, {1: 1.0}), store_and_fwd_flag_enc=SparseVector(2, {0: 1.0}), payment_type_enc=SparseVector(5, {0: 1.0}), rate_code_enc=SparseVector(12, {0: 1.0}), vector_categorico=SparseVector(21, {1: 1.0, 2: 1.0, 4: 1.0, 9: 1.0}), features=SparseVector(24, {0: -73.9431, 1: 40.7067, 2: 1.0, 4: 1.0, 5: 1.0, 7: 1.0, 12: 1.0}), prediccion=2.1970520795492154)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.transform(taxi).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos guardar el output a otro archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.9 ms, sys: 15.1 ms, total: 49 ms\n",
      "Wall time: 42.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(modelo\n",
    " .transform(taxi)\n",
    " .select(\n",
    "    variables_categoricas+variables_numericas+[\"prediccion\"]\n",
    " )\n",
    " .write\n",
    " .parquet(\"taxi_2014_prediccion\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_pred = spark.read.parquet(\"taxi_2014_prediccion/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(vendor_id='CMT', store_and_fwd_flag='N', payment_type='CRD', rate_code=1, pickup_longitude=-73.943106, pickup_latitude=40.706701, passenger_count=1, prediccion=2.1970520795492154)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(summary='count', prediccion='14999999'),\n",
       " Row(summary='mean', prediccion='1.4559072103899229'),\n",
       " Row(summary='stddev', prediccion='1.0965019704659398'),\n",
       " Row(summary='min', prediccion='0.2470778372101039'),\n",
       " Row(summary='max', prediccion='5.683756534354175')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_pred.select(\"prediccion\").describe().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validacion cruzada y Busquedas de hiperpar√°metros\n",
    "\n",
    "Spark es capaz de [validar y optimizar modelos](https://spark.apache.org/docs/latest/ml-tuning.html). Por desgracia no tiene la figura del RandomizedSearchCV, sin embargo si que tiene la busqueda en malla (`ParamGridBuilder`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [1e-3, 1.]) \\\n",
    "    .addGrid(lr.elasticNetParam, [1e-3, 1.]) \\\n",
    "    .build()\n",
    "type(paramGrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que hay 4 combinaciones distintas que va a buscar la malla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent='LinearRegression_2aa128e77d15', name='regParam', doc='regularization parameter (>= 0).'): 0.001,\n",
       "  Param(parent='LinearRegression_2aa128e77d15', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.001},\n",
       " {Param(parent='LinearRegression_2aa128e77d15', name='regParam', doc='regularization parameter (>= 0).'): 0.001,\n",
       "  Param(parent='LinearRegression_2aa128e77d15', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0},\n",
       " {Param(parent='LinearRegression_2aa128e77d15', name='regParam', doc='regularization parameter (>= 0).'): 1.0,\n",
       "  Param(parent='LinearRegression_2aa128e77d15', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.001},\n",
       " {Param(parent='LinearRegression_2aa128e77d15', name='regParam', doc='regularization parameter (>= 0).'): 1.0,\n",
       "  Param(parent='LinearRegression_2aa128e77d15', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paramGrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un evaluador de regression que sera el encargado de evaluar el funcionamiento de cada estimador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluador = RegressionEvaluator(metricName='rmse', predictionCol='prediccion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creamos un [`CrossValidator`](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) que se encargue de la validacion cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluador,\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.tuning.CrossValidator"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(crossval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora entrenamos la busqueda, esto tarda unos 35 minutos en mi ordenador!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.1 s, sys: 1.13 s, total: 5.24 s\n",
      "Wall time: 27min 29s\n"
     ]
    }
   ],
   "source": [
    "%time cvModel = crossval.fit(taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez ha terminado podemos ver los resultados de cada iteracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.7172700256296567,\n",
       " 1.7172883634559573,\n",
       " 1.7372954404496626,\n",
       " 2.1302285248602355]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y podemos ver los resultados de cada combinacion de hiperpar√°metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.7172700256296567,\n",
       "  {Param(parent='LinearRegression_2aa128e77d15', name='regParam', doc='regularization parameter (>= 0).'): 0.001,\n",
       "   Param(parent='LinearRegression_2aa128e77d15', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.001}),\n",
       " (1.7172883634559573,\n",
       "  {Param(parent='LinearRegression_2aa128e77d15', name='regParam', doc='regularization parameter (>= 0).'): 0.001,\n",
       "   Param(parent='LinearRegression_2aa128e77d15', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0}),\n",
       " (1.7372954404496626,\n",
       "  {Param(parent='LinearRegression_2aa128e77d15', name='regParam', doc='regularization parameter (>= 0).'): 1.0,\n",
       "   Param(parent='LinearRegression_2aa128e77d15', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.001}),\n",
       " (2.1302285248602355,\n",
       "  {Param(parent='LinearRegression_2aa128e77d15', name='regParam', doc='regularization parameter (>= 0).'): 1.0,\n",
       "   Param(parent='LinearRegression_2aa128e77d15', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 1.0})]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(cvModel.avgMetrics, paramGrid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos  ver los pasos del pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PipelineModel_2e66bde88a1c, LinearRegression_2aa128e77d15]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel.stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y finalmente, podemos ver los hiperparametros del mejor estimador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LinearRegression_2aa128e77d15', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2,\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.001,\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0.'): 1.35,\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='featuresCol', doc='features column name'): 'features',\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='fitIntercept', doc='whether to fit an intercept term'): True,\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='labelCol', doc='label column name'): 'label',\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber. (Default squaredError)'): 'squaredError',\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='maxIter', doc='maximum number of iterations (>= 0)'): 100,\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='predictionCol', doc='prediction column name'): 'prediccion',\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='regParam', doc='regularization parameter (>= 0)'): 0.001,\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (Default auto)'): 'auto',\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='standardization', doc='whether to standardize the training features before fitting the model'): True,\n",
       " Param(parent='LinearRegression_2aa128e77d15', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel.stages[1].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
